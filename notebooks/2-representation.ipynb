{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "## df = pd.read_csv(\"imdb_preprocessed.csv\") (remove the ## when trying to run because file too large to push to github)\n",
    "df['review_no_stop'] = df['review_no_stop'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choosing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'ten': 157, 'years': 174, 'since': 143, 'wildside': 172, 'aired': 10, 'nothing': 114, 'really': 128, 'come': 30, 'close': 28, 'quality': 124, 'local': 96, 'production': 122, 'includes': 83, 'two': 162, 'series': 137, 'enjoyable': 52, 'overrated': 118, 'underbelly': 164, 'brought': 22, 'life': 94, 'events': 58, 'recent': 129, 'criminal': 36, 'history': 76, 'sydney': 155, 'melbourne': 103, 'miniseries': 105, 'blue': 19, 'murder': 109, 'also': 13, 'starred': 148, 'tony': 161, 'martin': 100, 'someone': 144, 'side': 141, 'law': 90, 'may': 102, 'exceptionbr': 61, 'br': 21, 'currently': 37, 'repeated': 133, 'late': 88, 'night': 113, 'abc': 3, 'watched': 168, 'show': 140, 'quite': 125, 'im': 81, 'still': 149, 'impressed': 82, 'uncompromising': 163, 'story': 150, 'lines': 95, 'human': 80, 'characters': 26, 'cast': 25, 'excellent': 60, 'detective': 43, 'haunted': 73, 'disappearance': 46, 'son': 146, 'rachael': 126, 'blake': 18, 'later': 89, 'hooked': 78, 'real': 127, 'community': 31, 'worker': 173, 'struggling': 151, 'alcoholism': 11, 'alex': 12, 'dimitriades': 44, 'young': 176, 'cop': 32, 'whose': 171, 'vice': 166, 'gambling': 66, 'equally': 55, 'good': 70, 'support': 153, 'roles': 134, 'provided': 123, 'aaron': 1, 'pederson': 119, 'jessica': 86, 'napier': 112, 'mary': 101, 'coustas': 35, 'yes': 175, 'effie': 49, 'abbie': 2, 'cornishbr': 33, 'inexplicably': 84, 'released': 131, 'first': 63, 'three': 158, 'episodes': 54, 'dvd': 48, 'couple': 34, 'ago': 9, 'logic': 97, 'sort': 147, 'marketing': 99, 'beyond': 17, 'guessing': 72, 'something': 145, 'licensing': 93, 'disagreements': 45, 'original': 117, 'producersbr': 121, 'great': 71, 'aged': 8, 'remarkably': 132, 'well': 170, 'heres': 74, 'hoping': 79, 'abcs': 4, 'department': 42, 'gets': 67, 'act': 6, 'togetherbr': 160, 'according': 5, 'moderator': 107, 'message': 104, 'board': 20, 'release': 130, 'due': 47, 'december': 41, '2009': 0, 'betterthanaverage': 16, 'entry': 53, 'saint': 135, 'holds': 77, 'interest': 85, 'mysteries': 111, 'keeps': 87, 'end': 50, 'several': 139, 'suspects': 154, 'choose': 27, 'frombr': 65, 'many': 98, 'films': 62, 'golden': 69, 'age': 7, 'tastes': 156, 'especially': 56, 'younger': 177, 'viewers': 167, 'date': 38, 'clothing': 29, 'cars': 23, 'settings': 138, 'etc': 57, 'nowadays': 115, 'asks': 14, 'highball': 75, 'wears': 169, 'suit': 152, 'tie': 159, 'everywhere': 59, 'legal': 92, 'process': 120, 'much': 108, 'simpler': 142, 'must': 110, 'dearth': 40, 'lawyers': 91, 'back': 15, 'frankly': 64, 'value': 165, 'missing': 106, 'daysbr': 39, 'case': 24, 'go': 68, 'enjoy': 51, 'oldfashioned': 116, 'sense': 136}\n",
      "Count vectors:\n",
      " [[1 1 1 3 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 4 1 0 0 1 1 0 1 0 1 1 1 1 1 1\n",
      "  1 1 0 0 0 1 1 1 1 1 1 1 3 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1\n",
      "  1 1 1 0 1 0 1 1 1 2 1 1 1 0 1 0 1 1 1 0 0 1 2 1 1 1 0 1 3 1 2 1 1 1 0 1\n",
      "  0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 1 1 0 1\n",
      "  1 1 1 2 1 1 1 1 0 1 0 1 0 1 1 0 1 2 1 1 1 0 1 0 1 0 1 1 2 1 2 1 2 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 2 0 1 1 0 0 1 0 1 0 0 0 0 0 0\n",
      "  0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0\n",
      "  1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      "  2 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Word Counts with CountVectorizer (scikit-learn)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use two sample cleaned reviews (space-joined, no stop words)\n",
    "documents = [\n",
    "    \" \".join(df[\"review_no_stop\"].iloc[0]),\n",
    "    \" \".join(df[\"review_no_stop\"].iloc[1])\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(documents)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)    # dict termâ†’column index\n",
    "counts = vectorizer.transform(documents)\n",
    "print(\"Count vectors:\\n\", counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'ten': 157, 'years': 174, 'since': 143, 'wildside': 172, 'aired': 10, 'nothing': 114, 'really': 128, 'come': 30, 'close': 28, 'quality': 124, 'local': 96, 'production': 122, 'includes': 83, 'two': 162, 'series': 137, 'enjoyable': 52, 'overrated': 118, 'underbelly': 164, 'brought': 22, 'life': 94, 'events': 58, 'recent': 129, 'criminal': 36, 'history': 76, 'sydney': 155, 'melbourne': 103, 'miniseries': 105, 'blue': 19, 'murder': 109, 'also': 13, 'starred': 148, 'tony': 161, 'martin': 100, 'someone': 144, 'side': 141, 'law': 90, 'may': 102, 'exceptionbr': 61, 'br': 21, 'currently': 37, 'repeated': 133, 'late': 88, 'night': 113, 'abc': 3, 'watched': 168, 'show': 140, 'quite': 125, 'im': 81, 'still': 149, 'impressed': 82, 'uncompromising': 163, 'story': 150, 'lines': 95, 'human': 80, 'characters': 26, 'cast': 25, 'excellent': 60, 'detective': 43, 'haunted': 73, 'disappearance': 46, 'son': 146, 'rachael': 126, 'blake': 18, 'later': 89, 'hooked': 78, 'real': 127, 'community': 31, 'worker': 173, 'struggling': 151, 'alcoholism': 11, 'alex': 12, 'dimitriades': 44, 'young': 176, 'cop': 32, 'whose': 171, 'vice': 166, 'gambling': 66, 'equally': 55, 'good': 70, 'support': 153, 'roles': 134, 'provided': 123, 'aaron': 1, 'pederson': 119, 'jessica': 86, 'napier': 112, 'mary': 101, 'coustas': 35, 'yes': 175, 'effie': 49, 'abbie': 2, 'cornishbr': 33, 'inexplicably': 84, 'released': 131, 'first': 63, 'three': 158, 'episodes': 54, 'dvd': 48, 'couple': 34, 'ago': 9, 'logic': 97, 'sort': 147, 'marketing': 99, 'beyond': 17, 'guessing': 72, 'something': 145, 'licensing': 93, 'disagreements': 45, 'original': 117, 'producersbr': 121, 'great': 71, 'aged': 8, 'remarkably': 132, 'well': 170, 'heres': 74, 'hoping': 79, 'abcs': 4, 'department': 42, 'gets': 67, 'act': 6, 'togetherbr': 160, 'according': 5, 'moderator': 107, 'message': 104, 'board': 20, 'release': 130, 'due': 47, 'december': 41, '2009': 0, 'betterthanaverage': 16, 'entry': 53, 'saint': 135, 'holds': 77, 'interest': 85, 'mysteries': 111, 'keeps': 87, 'end': 50, 'several': 139, 'suspects': 154, 'choose': 27, 'frombr': 65, 'many': 98, 'films': 62, 'golden': 69, 'age': 7, 'tastes': 156, 'especially': 56, 'younger': 177, 'viewers': 167, 'date': 38, 'clothing': 29, 'cars': 23, 'settings': 138, 'etc': 57, 'nowadays': 115, 'asks': 14, 'highball': 75, 'wears': 169, 'suit': 152, 'tie': 159, 'everywhere': 59, 'legal': 92, 'process': 120, 'much': 108, 'simpler': 142, 'must': 110, 'dearth': 40, 'lawyers': 91, 'back': 15, 'frankly': 64, 'value': 165, 'missing': 106, 'daysbr': 39, 'case': 24, 'go': 68, 'enjoy': 51, 'oldfashioned': 116, 'sense': 136}\n",
      "IDF values: [1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.         1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.         1.40546511\n",
      " 1.         1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511]\n",
      "TF-IDF vector for first doc:\n",
      " [[0.07369347 0.07369347 0.07369347 0.22108042 0.07369347 0.07369347\n",
      "  0.07369347 0.         0.07369347 0.07369347 0.07369347 0.07369347\n",
      "  0.07369347 0.07369347 0.         0.         0.         0.07369347\n",
      "  0.07369347 0.07369347 0.07369347 0.20973406 0.07369347 0.\n",
      "  0.         0.07369347 0.07369347 0.         0.07369347 0.\n",
      "  0.07369347 0.07369347 0.07369347 0.07369347 0.07369347 0.07369347\n",
      "  0.07369347 0.07369347 0.         0.         0.         0.07369347\n",
      "  0.07369347 0.07369347 0.07369347 0.07369347 0.07369347 0.07369347\n",
      "  0.22108042 0.07369347 0.         0.         0.07369347 0.\n",
      "  0.07369347 0.07369347 0.         0.         0.07369347 0.\n",
      "  0.07369347 0.07369347 0.         0.07369347 0.         0.\n",
      "  0.07369347 0.07369347 0.         0.         0.05243351 0.07369347\n",
      "  0.05243351 0.07369347 0.07369347 0.         0.07369347 0.\n",
      "  0.07369347 0.07369347 0.07369347 0.14738695 0.07369347 0.07369347\n",
      "  0.07369347 0.         0.07369347 0.         0.07369347 0.07369347\n",
      "  0.07369347 0.         0.         0.07369347 0.14738695 0.07369347\n",
      "  0.07369347 0.07369347 0.         0.07369347 0.22108042 0.07369347\n",
      "  0.14738695 0.07369347 0.07369347 0.07369347 0.         0.07369347\n",
      "  0.         0.07369347 0.         0.         0.07369347 0.07369347\n",
      "  0.07369347 0.         0.         0.07369347 0.07369347 0.07369347\n",
      "  0.         0.07369347 0.07369347 0.07369347 0.07369347 0.07369347\n",
      "  0.07369347 0.07369347 0.07369347 0.07369347 0.07369347 0.07369347\n",
      "  0.07369347 0.07369347 0.07369347 0.         0.         0.10486703\n",
      "  0.         0.         0.07369347 0.07369347 0.         0.07369347\n",
      "  0.07369347 0.07369347 0.07369347 0.14738695 0.07369347 0.07369347\n",
      "  0.07369347 0.07369347 0.         0.07369347 0.         0.07369347\n",
      "  0.         0.07369347 0.07369347 0.         0.07369347 0.14738695\n",
      "  0.07369347 0.07369347 0.07369347 0.         0.07369347 0.\n",
      "  0.07369347 0.         0.07369347 0.07369347 0.14738695 0.07369347\n",
      "  0.14738695 0.07369347 0.14738695 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Word Frequencies with TfidfVectorizer (scikit-learn)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(documents)\n",
    "\n",
    "print(\"Vocabulary:\", tfidf.vocabulary_)\n",
    "print(\"IDF values:\", tfidf.idf_)\n",
    "\n",
    "tfidf_vec = tfidf.transform([documents[0]])\n",
    "print(\"TF-IDF vector for first doc:\\n\", tfidf_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 40000 rows; Test: 10000 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load a preprocessed file\n",
    "## df = pd.read_csv(\"imdb_preprocessed.csv\") (remove the ## when run because file too big to upload to github)\n",
    "\n",
    "# Split into train/test\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,   \n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]  # keep label balance\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(df_train), \"rows; Test:\", len(df_test), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-join precomputed tokens into raw strings\n",
    "train_corpus = [\" \".join(ast.literal_eval(toks)) for toks in df_train[\"review_no_stop\"]]\n",
    "test_corpus  = [\" \".join(ast.literal_eval(toks)) for toks in df_test [\"review_no_stop\"]]\n",
    "\n",
    "# TF-IDF vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_train = tfidf.fit_transform(train_corpus)\n",
    "X_test  = tfidf.transform(test_corpus)\n",
    "\n",
    "# Labels\n",
    "y_train = df_train[\"label\"]\n",
    "y_test  = df_test [\"label\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
