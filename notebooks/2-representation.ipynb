{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./1-preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choosing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'ten': 157, 'years': 174, 'since': 143, 'wildside': 172, 'aired': 10, 'nothing': 114, 'really': 128, 'come': 30, 'close': 28, 'quality': 124, 'local': 96, 'production': 122, 'includes': 83, 'two': 162, 'series': 137, 'enjoyable': 52, 'overrated': 118, 'underbelly': 164, 'brought': 22, 'life': 94, 'events': 58, 'recent': 129, 'criminal': 36, 'history': 76, 'sydney': 155, 'melbourne': 103, 'miniseries': 105, 'blue': 19, 'murder': 109, 'also': 13, 'starred': 148, 'tony': 161, 'martin': 100, 'someone': 144, 'side': 141, 'law': 90, 'may': 102, 'exceptionbr': 61, 'br': 21, 'currently': 37, 'repeated': 133, 'late': 88, 'night': 113, 'abc': 3, 'watched': 168, 'show': 140, 'quite': 125, 'im': 81, 'still': 149, 'impressed': 82, 'uncompromising': 163, 'story': 150, 'lines': 95, 'human': 80, 'characters': 26, 'cast': 25, 'excellent': 60, 'detective': 43, 'haunted': 73, 'disappearance': 46, 'son': 146, 'rachael': 126, 'blake': 18, 'later': 89, 'hooked': 78, 'real': 127, 'community': 31, 'worker': 173, 'struggling': 151, 'alcoholism': 11, 'alex': 12, 'dimitriades': 44, 'young': 176, 'cop': 32, 'whose': 171, 'vice': 166, 'gambling': 66, 'equally': 55, 'good': 70, 'support': 153, 'roles': 134, 'provided': 123, 'aaron': 1, 'pederson': 119, 'jessica': 86, 'napier': 112, 'mary': 101, 'coustas': 35, 'yes': 175, 'effie': 49, 'abbie': 2, 'cornishbr': 33, 'inexplicably': 84, 'released': 131, 'first': 63, 'three': 158, 'episodes': 54, 'dvd': 48, 'couple': 34, 'ago': 9, 'logic': 97, 'sort': 147, 'marketing': 99, 'beyond': 17, 'guessing': 72, 'something': 145, 'licensing': 93, 'disagreements': 45, 'original': 117, 'producersbr': 121, 'great': 71, 'aged': 8, 'remarkably': 132, 'well': 170, 'heres': 74, 'hoping': 79, 'abcs': 4, 'department': 42, 'gets': 67, 'act': 6, 'togetherbr': 160, 'according': 5, 'moderator': 107, 'message': 104, 'board': 20, 'release': 130, 'due': 47, 'december': 41, '2009': 0, 'betterthanaverage': 16, 'entry': 53, 'saint': 135, 'holds': 77, 'interest': 85, 'mysteries': 111, 'keeps': 87, 'end': 50, 'several': 139, 'suspects': 154, 'choose': 27, 'frombr': 65, 'many': 98, 'films': 62, 'golden': 69, 'age': 7, 'tastes': 156, 'especially': 56, 'younger': 177, 'viewers': 167, 'date': 38, 'clothing': 29, 'cars': 23, 'settings': 138, 'etc': 57, 'nowadays': 115, 'asks': 14, 'highball': 75, 'wears': 169, 'suit': 152, 'tie': 159, 'everywhere': 59, 'legal': 92, 'process': 120, 'much': 108, 'simpler': 142, 'must': 110, 'dearth': 40, 'lawyers': 91, 'back': 15, 'frankly': 64, 'value': 165, 'missing': 106, 'daysbr': 39, 'case': 24, 'go': 68, 'enjoy': 51, 'oldfashioned': 116, 'sense': 136}\n",
      "Count vectors:\n",
      " [[1 1 1 3 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 4 1 0 0 1 1 0 1 0 1 1 1 1 1 1\n",
      "  1 1 0 0 0 1 1 1 1 1 1 1 3 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1\n",
      "  1 1 1 0 1 0 1 1 1 2 1 1 1 0 1 0 1 1 1 0 0 1 2 1 1 1 0 1 3 1 2 1 1 1 0 1\n",
      "  0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 1 1 0 1\n",
      "  1 1 1 2 1 1 1 1 0 1 0 1 0 1 1 0 1 2 1 1 1 0 1 0 1 0 1 1 2 1 2 1 2 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 2 0 1 1 0 0 1 0 1 0 0 0 0 0 0\n",
      "  0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0\n",
      "  1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      "  2 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Word Counts with CountVectorizer (scikit-learn)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use two sample cleaned reviews (space-joined, no stop words)\n",
    "documents = [\n",
    "    \" \".join(df[\"review_no_stop\"].iloc[0]),\n",
    "    \" \".join(df[\"review_no_stop\"].iloc[1])\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(documents)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)    # dict termâ†’column index\n",
    "counts = vectorizer.transform(documents)\n",
    "print(\"Count vectors:\\n\", counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'ten': 157, 'years': 174, 'since': 143, 'wildside': 172, 'aired': 10, 'nothing': 114, 'really': 128, 'come': 30, 'close': 28, 'quality': 124, 'local': 96, 'production': 122, 'includes': 83, 'two': 162, 'series': 137, 'enjoyable': 52, 'overrated': 118, 'underbelly': 164, 'brought': 22, 'life': 94, 'events': 58, 'recent': 129, 'criminal': 36, 'history': 76, 'sydney': 155, 'melbourne': 103, 'miniseries': 105, 'blue': 19, 'murder': 109, 'also': 13, 'starred': 148, 'tony': 161, 'martin': 100, 'someone': 144, 'side': 141, 'law': 90, 'may': 102, 'exceptionbr': 61, 'br': 21, 'currently': 37, 'repeated': 133, 'late': 88, 'night': 113, 'abc': 3, 'watched': 168, 'show': 140, 'quite': 125, 'im': 81, 'still': 149, 'impressed': 82, 'uncompromising': 163, 'story': 150, 'lines': 95, 'human': 80, 'characters': 26, 'cast': 25, 'excellent': 60, 'detective': 43, 'haunted': 73, 'disappearance': 46, 'son': 146, 'rachael': 126, 'blake': 18, 'later': 89, 'hooked': 78, 'real': 127, 'community': 31, 'worker': 173, 'struggling': 151, 'alcoholism': 11, 'alex': 12, 'dimitriades': 44, 'young': 176, 'cop': 32, 'whose': 171, 'vice': 166, 'gambling': 66, 'equally': 55, 'good': 70, 'support': 153, 'roles': 134, 'provided': 123, 'aaron': 1, 'pederson': 119, 'jessica': 86, 'napier': 112, 'mary': 101, 'coustas': 35, 'yes': 175, 'effie': 49, 'abbie': 2, 'cornishbr': 33, 'inexplicably': 84, 'released': 131, 'first': 63, 'three': 158, 'episodes': 54, 'dvd': 48, 'couple': 34, 'ago': 9, 'logic': 97, 'sort': 147, 'marketing': 99, 'beyond': 17, 'guessing': 72, 'something': 145, 'licensing': 93, 'disagreements': 45, 'original': 117, 'producersbr': 121, 'great': 71, 'aged': 8, 'remarkably': 132, 'well': 170, 'heres': 74, 'hoping': 79, 'abcs': 4, 'department': 42, 'gets': 67, 'act': 6, 'togetherbr': 160, 'according': 5, 'moderator': 107, 'message': 104, 'board': 20, 'release': 130, 'due': 47, 'december': 41, '2009': 0, 'betterthanaverage': 16, 'entry': 53, 'saint': 135, 'holds': 77, 'interest': 85, 'mysteries': 111, 'keeps': 87, 'end': 50, 'several': 139, 'suspects': 154, 'choose': 27, 'frombr': 65, 'many': 98, 'films': 62, 'golden': 69, 'age': 7, 'tastes': 156, 'especially': 56, 'younger': 177, 'viewers': 167, 'date': 38, 'clothing': 29, 'cars': 23, 'settings': 138, 'etc': 57, 'nowadays': 115, 'asks': 14, 'highball': 75, 'wears': 169, 'suit': 152, 'tie': 159, 'everywhere': 59, 'legal': 92, 'process': 120, 'much': 108, 'simpler': 142, 'must': 110, 'dearth': 40, 'lawyers': 91, 'back': 15, 'frankly': 64, 'value': 165, 'missing': 106, 'daysbr': 39, 'case': 24, 'go': 68, 'enjoy': 51, 'oldfashioned': 116, 'sense': 136}\n",
      "IDF values: [1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.         1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.         1.40546511\n",
      " 1.         1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511]\n",
      "TF-IDF vector for first doc:\n",
      " [[0.07369347 0.07369347 0.07369347 0.22108042 0.07369347 0.07369347\n",
      "  0.07369347 0.         0.07369347 0.07369347 0.07369347 0.07369347\n",
      "  0.07369347 0.07369347 0.         0.         0.         0.07369347\n",
      "  0.07369347 0.07369347 0.07369347 0.20973406 0.07369347 0.\n",
      "  0.         0.07369347 0.07369347 0.         0.07369347 0.\n",
      "  0.07369347 0.07369347 0.07369347 0.07369347 0.07369347 0.07369347\n",
      "  0.07369347 0.07369347 0.         0.         0.         0.07369347\n",
      "  0.07369347 0.07369347 0.07369347 0.07369347 0.07369347 0.07369347\n",
      "  0.22108042 0.07369347 0.         0.         0.07369347 0.\n",
      "  0.07369347 0.07369347 0.         0.         0.07369347 0.\n",
      "  0.07369347 0.07369347 0.         0.07369347 0.         0.\n",
      "  0.07369347 0.07369347 0.         0.         0.05243351 0.07369347\n",
      "  0.05243351 0.07369347 0.07369347 0.         0.07369347 0.\n",
      "  0.07369347 0.07369347 0.07369347 0.14738695 0.07369347 0.07369347\n",
      "  0.07369347 0.         0.07369347 0.         0.07369347 0.07369347\n",
      "  0.07369347 0.         0.         0.07369347 0.14738695 0.07369347\n",
      "  0.07369347 0.07369347 0.         0.07369347 0.22108042 0.07369347\n",
      "  0.14738695 0.07369347 0.07369347 0.07369347 0.         0.07369347\n",
      "  0.         0.07369347 0.         0.         0.07369347 0.07369347\n",
      "  0.07369347 0.         0.         0.07369347 0.07369347 0.07369347\n",
      "  0.         0.07369347 0.07369347 0.07369347 0.07369347 0.07369347\n",
      "  0.07369347 0.07369347 0.07369347 0.07369347 0.07369347 0.07369347\n",
      "  0.07369347 0.07369347 0.07369347 0.         0.         0.10486703\n",
      "  0.         0.         0.07369347 0.07369347 0.         0.07369347\n",
      "  0.07369347 0.07369347 0.07369347 0.14738695 0.07369347 0.07369347\n",
      "  0.07369347 0.07369347 0.         0.07369347 0.         0.07369347\n",
      "  0.         0.07369347 0.07369347 0.         0.07369347 0.14738695\n",
      "  0.07369347 0.07369347 0.07369347 0.         0.07369347 0.\n",
      "  0.07369347 0.         0.07369347 0.07369347 0.14738695 0.07369347\n",
      "  0.14738695 0.07369347 0.14738695 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Word Frequencies with TfidfVectorizer (scikit-learn)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(documents)\n",
    "\n",
    "print(\"Vocabulary:\", tfidf.vocabulary_)\n",
    "print(\"IDF values:\", tfidf.idf_)\n",
    "\n",
    "tfidf_vec = tfidf.transform([documents[0]])\n",
    "print(\"TF-IDF vector for first doc:\\n\", tfidf_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>no_punct</th>\n",
       "      <th>tokens</th>\n",
       "      <th>review_no_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>I rented I AM CURIOUSYELLOW from my video stor...</td>\n",
       "      <td>[i, rented, i, am, curiousyellow, from, my, vi...</td>\n",
       "      <td>[rented, curiousyellow, video, store, controve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>I Am Curious Yellow is a risible and pretentio...</td>\n",
       "      <td>[i, am, curious, yellow, is, a, risible, and, ...</td>\n",
       "      <td>[curious, yellow, risible, pretentious, steami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>[if, only, to, avoid, making, this, type, of, ...</td>\n",
       "      <td>[avoid, making, type, film, future, film, inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>This film was probably inspired by Godards Mas...</td>\n",
       "      <td>[this, film, was, probably, inspired, by, goda...</td>\n",
       "      <td>[film, probably, inspired, godards, masculin, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>Oh brotherafter hearing about this ridiculous ...</td>\n",
       "      <td>[oh, brotherafter, hearing, about, this, ridic...</td>\n",
       "      <td>[oh, brotherafter, hearing, ridiculous, film, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...   \n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...   \n",
       "2  If only to avoid making this type of film in t...   \n",
       "3  This film was probably inspired by Godard's Ma...   \n",
       "4  Oh, brother...after hearing about this ridicul...   \n",
       "\n",
       "                                            no_punct  \\\n",
       "0  I rented I AM CURIOUSYELLOW from my video stor...   \n",
       "1  I Am Curious Yellow is a risible and pretentio...   \n",
       "2  If only to avoid making this type of film in t...   \n",
       "3  This film was probably inspired by Godards Mas...   \n",
       "4  Oh brotherafter hearing about this ridiculous ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [i, rented, i, am, curiousyellow, from, my, vi...   \n",
       "1  [i, am, curious, yellow, is, a, risible, and, ...   \n",
       "2  [if, only, to, avoid, making, this, type, of, ...   \n",
       "3  [this, film, was, probably, inspired, by, goda...   \n",
       "4  [oh, brotherafter, hearing, about, this, ridic...   \n",
       "\n",
       "                                      review_no_stop  \n",
       "0  [rented, curiousyellow, video, store, controve...  \n",
       "1  [curious, yellow, risible, pretentious, steami...  \n",
       "2  [avoid, making, type, film, future, film, inte...  \n",
       "3  [film, probably, inspired, godards, masculin, ...  \n",
       "4  [oh, brotherafter, hearing, ridiculous, film, ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Grab IMDB from HF\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "train_df = pd.DataFrame(ds[\"train\"])\n",
    "test_df  = pd.DataFrame(ds[\"test\"])\n",
    "\n",
    "# Apply the same remove-punctuation, tokenize, remove-stopwords\n",
    "train_df[\"no_punct\"] = train_df[\"text\"].apply(remove_punctuation)\n",
    "train_df[\"tokens\"] = train_df[\"no_punct\"].apply(tokenize)\n",
    "train_df[\"review_no_stop\"] = train_df[\"tokens\"].apply(remove_stopwords)\n",
    "\n",
    "# Quick check:\n",
    "train_df[[\"text\", \"no_punct\", \"tokens\", \"review_no_stop\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-join precomputed tokens into raw strings\n",
    "train_corpus = [\" \".join(ast.literal_eval(toks)) for toks in df_train[\"review_no_stop\"]]\n",
    "test_corpus  = [\" \".join(ast.literal_eval(toks)) for toks in df_test [\"review_no_stop\"]]\n",
    "\n",
    "# TF-IDF vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_train = tfidf.fit_transform(train_corpus)\n",
    "X_test  = tfidf.transform(test_corpus)\n",
    "\n",
    "# Labels\n",
    "y_train = df_train[\"label\"]\n",
    "y_test  = df_test [\"label\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
